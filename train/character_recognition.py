# -*- coding: utf-8 -*-
"""Character_Recognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d2NqRKeMwZoA2EAIiutA6El2qIsZ81ly
"""

import tensorflow as tf
from keras.datasets import mnist
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Activation
from keras.layers.convolutional import Conv2D, MaxPooling2D
from tensorflow.keras.layers import BatchNormalization
import keras
import keras.utils
from keras.utils import np_utils

!git clone https://github.com/machinecurve/extra_keras_datasets.git

from extra_keras_datasets.extra_keras_datasets import emnist
(X_train, y_train), (X_test, y_test) = emnist.load_data(type='byclass')

plt.imshow(X_train[23])

X_train = np.expand_dims(X_train,-1)
X_test = np.expand_dims(X_test, -1)

print(X_train[0])

X_train = X_train.astype(np.float32)/255
X_test = X_test.astype(np.float32)/255

print(X_train[0])

nb_classes = 62 
Y_train = np_utils.to_categorical(y_train, nb_classes)
Y_test = np_utils.to_categorical(y_test, nb_classes)

print(Y_train)

print("X_Train: "+ str(X_train.shape))
print("Y_Train: "+ str(y_train.shape))
print("X_Test: "+ str(X_test.shape))
print("X_Test: "+ str(y_test.shape))

print(X_train[0])

plt.imshow(X_train[3], cmap = 'binary')
print(y_train[3])

model = tf.keras.Sequential([ 
    tf.keras.layers.Conv2D(32,3,input_shape=(28,28,1)),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Flatten(input_shape=(28,28,1)),
    tf.keras.layers.Dense(512,activation='relu'),
    tf.keras.layers.Dense(128,activation='relu'),
    tf.keras.layers.Dense(nb_classes,activation='softmax')
])
model.summary()

"""Conversion to 1D Array

Model Summary
"""

model = Sequential()                                 # Linear stacking of layers

# Convolution Layer 1
model.add(Conv2D(62, (3, 3), input_shape=(28,28,1))) # 32 different 3x3 kernels -- so 32 feature maps
model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation
convLayer01 = Activation('relu')                     # activation
model.add(convLayer01)

# Convolution Layer 2
model.add(Conv2D(62, (3, 3)))                        # 32 different 3x3 kernels -- so 32 feature maps
model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation
model.add(Activation('relu'))                        # activation
convLayer02 = MaxPooling2D(pool_size=(2,2))          # Pool the max values over a 2x2 kernel
model.add(convLayer02)

# Convolution Layer 3
model.add(Conv2D(124,(3, 3)))                         # 64 different 3x3 kernels -- so 64 feature maps
model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation
convLayer03 = Activation('relu')                     # activation
model.add(convLayer03)

# Convolution Layer 4
model.add(Conv2D(62, (3, 3)))                        # 64 different 3x3 kernels -- so 64 feature maps
model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation
model.add(Activation('relu'))                        # activation
convLayer04 = MaxPooling2D(pool_size=(2,2))          # Pool the max values over a 2x2 kernel
model.add(convLayer04)
model.add(Flatten())                                 # Flatten final 4x4x64 output matrix into a 1024-length vector

# Fully Connected Layer 5
model.add(Dense(496))                                # 512 FCN nodes
model.add(BatchNormalization())                      # normalization
model.add(Activation('relu'))                        # activation

# Fully Connected Layer 6                       
model.add(Dropout(0.2))                              # 20% dropout of randomly selected nodes
model.add(Dense(62))                                 # final 10 FCN nodes
model.add(Activation('softmax'))                     # softmax activation

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(X_train, Y_train,
          batch_size=128, epochs=10,
          verbose=1)

from google.colab import drive
drive.mount('/content/drive')

model.save('/content/drive/MyDrive/Character_Rec.h5')

model = keras.models.load_model('/content/drive/MyDrive/Character_Rec.h5', compile=True)

from keras.models import model_from_json   
# serialize model to JSON
model_json = model.to_json()
with open("/content/drive/MyDrive/CDR_Model/model.json", "w") as json_file:
    json_file.write(model_json)
# serialize weights to HDF5
model.save_weights("/content/drive/MyDrive/CDR_Model/weights.hdf5")
print("Saved model to GDrive")

model.evaluate(X_test, y_test)

"""# New Section"""

model.predict(X_test)

predict_x=model.predict(X_test) 
classes_x=np.argmax(predict_x,axis=1)

print(classes_x)

y_test[-3]